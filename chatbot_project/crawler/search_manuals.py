from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

def search_manuals(keyword: str, max_results: int = 10) -> list:
    chrome_options = Options()
    # chrome_options.add_argument("--headless")  # UI ì—†ì´ ì‹¤í–‰
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("user-agent=Mozilla/5.0 ...")

    driver = webdriver.Chrome(options=chrome_options)

    try:
        search_url = f"https://manuals.plus/?s={keyword.replace(' ', '+')}"
        driver.get(search_url)
        time.sleep(3)

        with open("search_result.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)

        result_links = driver.find_elements(By.CSS_SELECTOR, "h2.entry-title a")
        manual_links = []

        for link in result_links[:max_results]:
            href = link.get_attribute("href")
            print(f"ğŸ”— ìƒì„¸ í˜ì´ì§€: {href}")
            driver.get(href)
            time.sleep(2)

            a_tags = driver.find_elements(By.TAG_NAME, "a")
            for a in a_tags:
                pdf = a.get_attribute("href")
                if pdf and pdf.endswith(".pdf"):
                    manual_links.append(pdf)
                    print(f"âœ… PDF ë§í¬ ë°œê²¬: {pdf}")
                    break

        return manual_links

    finally:
        driver.quit()


'''
requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ë°©ë²• 
í•˜ì§€ë§Œ ì›¹ì‚¬ì´íŠ¸ê°€ ë¹„ë¸Œë¼ìš°ì € í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ ì°¨ë‹¨í•˜ê³  ìˆì–´ì„œ 403 Forbidden Error ë°œìƒ
headerì— User agentë¥¼ ì¶”ê°€í•´ì„œ ìš°íšŒ ì‹œë„í–ˆìœ¼ë‚˜ ê·¸ê²ƒì¡°ì°¨ ì°¨ë‹¨ë¨
'''

# import requests
# from bs4 import BeautifulSoup

# def search_manual_links(keyword: str, max_results: int = 10) -> list:
#     """
#     manuals.plusì˜ ê²€ìƒ‰ APIë¥¼ ì´ìš©í•´ keywordì— í•´ë‹¹í•˜ëŠ” ë§¤ë‰´ì–¼ ìƒì„¸ í˜ì´ì§€ë¥¼ ì°¾ê³ ,
#     ê° í˜ì´ì§€ì—ì„œ PDF ë§í¬ë¥¼ ì¶”ì¶œí•´ ë°˜í™˜í•œë‹¤.
#     """
#     api_url = "https://manuals.plus/wp-json/wp/v2/search"
#     params = {
#         "search": keyword,
#         "_embed": "true"
#     }

#     try:
#         headers = {
#             "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
#         }

#         res = requests.get(api_url, params=params, headers=headers, timeout=10)
#         res.raise_for_status()
#         search_results = res.json()
#     except Exception as e:
#         print(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {e}")
#         return []

#     pdf_links = []

#     for item in search_results[:max_results]:
#         page_url = item.get("url")
#         print(f"ğŸ” ë§¤ë‰´ì–¼ í˜ì´ì§€: {page_url}")

#         try:
#             page_res = requests.get(page_url, timeout=10)
#             page_res.raise_for_status()
#             soup = BeautifulSoup(page_res.text, "html.parser")

#             for a in soup.find_all("a", href=True):
#                 href = a["href"]
#                 if href.lower().endswith(".pdf"):
#                     pdf_links.append(href)
#                     print(f"PDF ë§í¬ ë°œê²¬: {href}")
#                     break  # í˜ì´ì§€ë‹¹ í•˜ë‚˜ë§Œ ìˆ˜ì§‘

#         except Exception as e:
#             print(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {page_url} â†’ {e}")

#     return pdf_links