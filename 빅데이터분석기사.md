# 제1 유형

### Pandas 기본

import pandas as pd

df = pd.read_csv("/file/path.csv")
Ans = df.head(5)
df.shape
df.info()

df[ 조건식 or 컬럼 ]

.iloc[row, column] <- 2차원 행렬 탐색 (인덱스 기반)
.loc[row, column]  <- 2차원 행렬 탐색 (행렬 이름 기반)
.index[] <- 인덱스 출력

.idxmax() <- 값이 최대인 행의 인덱스 탐색 

.sum() <- 총합
.cumsum() <- 누적합
.value_counts() <- 총 개수

.mean() <- 평균
.median() <- 중앙값
.var() <- 분산
.std() <- 표준편차
.max() <- 최댓값
.min() <- 최솟값
.mode()[n] <- n번째 최빈값

.skew() <- 왜도
.kurt() <- 첨도 

### 데이터 삭제 및 대체
.drop([column], axis=1) <- 컬럼 삭제
.dropna(subset=['column']) <- 결측치 삭제 (subset에 지정한 column에 결측값이 있으면 행 삭제)
.fillna(value) <- 결측치 채우기
.fillna(method = 'bfill (다음값)' / 'pad (이전값)')

.replace('value1', 'value2') <- value1 을 value2 로 변경 

예시 -> df['f1'] = df['f1'].fillna(df['city'].map({'서울':s,'경기':k,'부산':b,'대구':d}))

.unique() <- 고유값

### 이상치 탐색 (IQR)

df.isnull().sum()  <- 각 칼럼의 결측값 개수 확인

Q1 = df['column'].quantile(0.25)  <- 특정 컬럼의 사분위수 확인

IQR = Q3 - Q1
Min = Q1 - 1.5*IQR
Max = Q3 + 1.5*IQR

out1 = df[df['Fare'] < Min]
out2 = df[df['Fare'] > Max]
outlier = pd.concat([out1, out2])

m_ceil = np.ceil(df['age'])  <- 올림
m_floor = np.floor(df['age'])  <- 내림
m_trunc = np.trunc(df['age'])  <- 버림

df.isnull().sum()/df.shape[0] <- 결측치 비율

np.log1p(df[column]) <- 컬럼 로그변환 

### 그룹화
df.groupby(['column1', 'column2']) <- 각 컬럼 별로 그룹화

그룹별 평균	df.groupby('col')['val'].mean()
그룹별 최대	df.groupby('col')['val'].max()
그룹별 여러 집계	agg(['mean', 'max', 'count'])
그룹별 인덱스 추출	idxmax(), idxmin()
그룹별 일부 행 추출	groupby().head(n)

pd.melt(df, id_vars=['column'], value_vars=['column1', 'column2']) <- 세로로 녹여서 재구성
id로 사용할 열 지정, 녹여서 variable로 사용할 열 지정

### 정규화
from sklearn.preprocessing import StandardScaler, power_transform, MinMaxScaler

scaler - StandardScaler() <- Z-score 정규화 (표준화)
scaler = MinMaxScaler() <- min-max 스케일 변환

df[column] = scaler.fit_transform(df[[column]])

df[column] = power_transform(df[[column]]) <- yeo-johnson 변환
df[column] = power_transform(df[[column]], method='box-cox) <- box-cox 변환 

scaler = MinMaxScaler()

### 정렬
df.sort_values(column, ascending = False) <- 내림차순 정렬 

df.reset_index(drop=True) <- dataframe 전환 (기존 인덱스 제거 후 0부터 다시 인덱스 부여)

### 상관관계
.corr()

### 시계열 데이터
pd.to_datetime(df['Date'], format='%Y%m%d') <- object type을 datetime64 type으로 변경
.year <- 년
.month <- 월
.day <- 일
.weekday <- 요일
.hour <- 시간
.minute <- 분
.second <- 초
.date <- 날짜
.time <- 시간

.strftime('%Y-%m-%d %H:%M:%S') <- datetime 객체를 원하는 포맷으로 변환 
.dt.total_seconds() <- 초 단위로 시간 계산

df['dayofweek'] = df['Date'].dt.dayofweek
df['weekend'] = df['dayofweek'].apply(lambda x: x>=5)

df = df.apply(lambda x: event_sales(x), axis=1)  <- axis = 0 (열 단위) / axis = 1 (행 단위)

df_w = df.resample('W').sum() <- 주 단위로 샘플링 (W : 주 단위 / 2W : 2주 단위 / M : 월 단위)

### 데이터 병합
b1 = pd.read_csv("../input/bigdatacertificationkr/basic1.csv")
b3 = pd.read_csv("../input/bigdatacertificationkr/basic3.csv")
df = pd.merge(left = b1 , right = b3, how = "left", on = "f4")

### 구간 분할
pd.qcut(df['age'], q=3)
df['range'] = pd.qcut(df['age'], q=3, labels=['group1','group2','group3'])

### 중복 제거
.drop_duplicates(subset=['age'])

### 한 칸 앞으로
df['previous_PV'] = df['PV'].shift(1)
.shift(n) <- 값을 아래로 n칸 이동 시킴 (n이 음수라면 위로 이동) 이때 몇몇 칸은 결측치가 생성됨

### 문자열
.str[] <- 인덱스 슬라이싱 방법 
.contains() <- 문자열 포함 확인


# 제 2유형

1. 문제 정의 

- 분류 / 회귀 어떤 건지 판단
- 그룹 나누기 / 확률예측 -> 분류 / 가격예측 -> 회귀 
- 평가 지표 칼럼 확인 : train 데이터에 있고 test에는 없는 칼럼
- 제출할 csv 파일의 형태 확인 (인덱스 유무, 칼럼수, 칼럼명)

2. 데이터 불러오기

3. EDA : 탐색적 데이터 분석

- head(), shape()로 칼럼 갯수와 값 확인
- info()로 시계열 데이터가 object인지 datetime인지 확인
- describe(include="O")를 통해 수치형/범주형 간단한 통계값 확인, 이상치 확인
- isnull().sum()을 통해 결측치 존재 여부 확인해서 .fillna()로 대처 (.mode[0] / .median() / .mean())
- 절대 결측치 있는 행 데이터 삭제하지 말것

4. 데이터 전처리

- 인덱스 같은 불필요한 column은 dataset.drop('column', axis=1) 로 제거
- 제출파일에 id가 필요한 경우 또는 train 데이터의 target 칼럼 : pop('column')을 사용
- 원핫 인코딩, 라벨 인코더 사용방법

5. 검증 데이터 분리

x_tr, x_val, y_tr, y_val = train_test_split(train, target, test_size = 0.2, random_state = 0)

6. 랜덤포레스트

    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    model = RandomForestClassifier(random_state = 0)
    model.fit(x_tr, y_tr)
    pred = model.predict(x_val)

classifier : 분류 / regressor : 회귀

그 외 모델 종류
    from sklearn.linear_model import LogisticRegression, LinearRegression
    lr = LogisticRegression()
    lr.fit(x_tr, y_tr)
    pred = lr.predict(x_val)

    import xgboost as xgb
    xg = xgb.XGBRegressor(random_state=0)
    xg = xgb.XGBClassifier(random_state=0)

7. 평가 지표

검증 데이터로 대강 평가지표 확인
- 평가 지표는 대부분 sklearn.metrics 밑에 있음 : help/dir 적극 활용
- 분류 : accuracy_score, roc_auc_score, f1_score(micro/macro/weighted), precision_score(micro/macro/weighted), recall_score(micro/macro/weighted)
- 회귀 : mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error

- RMSE는 별도 패키지가 없어서 **0.5를 따로 해줘야함

    from sklearn.metrics import mean_squared_error

    rmse = (mean_squared_error(y_true, y_pred))**0.5

8. 테스트 데이터 예측 -> 파일 생성

    pred = model.predict(test)
    submission = pd.DataFrame({'pred' : pred}).to_csv('submission.csv', index=False)

    pd.read_csv('submission.csv'). <- 제출 전 확인

- 실제 테스트 데이터를 선정한 모델에 집어넣어 pred 데이터를 새로 생성하고 그 데이터가 테스트 데이터와 행 수가 똑같은지 확인
- 분류-확률 값 예측 모델은 predict_proba
- 칼럼명 / 인덱스 여부 등 문제 잘 확인해 csv 파일 만들고, 제출 전 확인
- 하이퍼파라미터 튜닝은 시간 남으면 가능


